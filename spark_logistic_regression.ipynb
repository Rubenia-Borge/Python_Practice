{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark for Machine Learning -Logistic Regression\n",
    "# Import Spark, create spark session and load the data from our bucket\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('assignment10').getOrCreate()\n",
    "data = spark.read.csv('gs://bucketassignment10/customer_churn.csv',inferSchema=True,\n",
    "                     header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema of our spark dataframe\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "|summary|        Names|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|            Location|             Company|              Churn|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "|  count|          900|              900|              900|               900|              900|               900|                 900|                 900|                900|\n",
      "|   mean|         null|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|                null|                null|0.16666666666666666|\n",
      "| stddev|         null|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969|                null|                null| 0.3728852122772358|\n",
      "|    min|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|00103 Jeffrey Cre...|     Abbott-Thompson|                  0|\n",
      "|    max|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|Unit 9800 Box 287...|Zuniga, Clark and...|                  1|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Shows a summary of descriptive statistics for our spark dataframe\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the data columns of our dataframe\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VectorAssembler from the Spark Machine Learning submodule.\n",
    "# VectorAssembler takes all the columns and combines them into a new vector column\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Pick numeric values columns\n",
    "assembler = VectorAssembler(inputCols=['Age',\n",
    " 'Total_Purchase',\n",
    " 'Account_Manager',\n",
    " 'Years',\n",
    " 'Num_Sites'],outputCol='features')\n",
    "    \n",
    "output = assembler.transform(data)\n",
    "final_data = output.select('features','churn')\n",
    "\n",
    "# Divide dataset into training set and test set\n",
    "# 70% of data for training set\n",
    "# 30% of data for test set\n",
    "train_churn,test_churn = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|              churn|         prediction|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|                664|                664|\n",
      "|   mean|0.17018072289156627|0.12349397590361445|\n",
      "| stddev|0.37607478620132123| 0.3292513881293036|\n",
      "|    min|                0.0|                0.0|\n",
      "|    max|                1.0|                1.0|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the logistic regression model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr_churn = LogisticRegression(labelCol='churn')\n",
    "fitted_churn_model = lr_churn.fit(train_churn)\n",
    "training_sum = fitted_churn_model.summary\n",
    "training_sum.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[26.0,8939.61,0.0...|    0|[6.47264016361498...|[0.99845724285900...|       0.0|\n",
      "|[29.0,9617.59,0.0...|    0|[4.56636606490787...|[0.98971128967545...|       0.0|\n",
      "|[29.0,13240.01,1....|    0|[6.57903920044534...|[0.99861274377268...|       0.0|\n",
      "|[29.0,13255.05,1....|    0|[4.01184985649093...|[0.98222189945838...|       0.0|\n",
      "|[30.0,8677.28,1.0...|    0|[3.93877984688534...|[0.98089995624143...|       0.0|\n",
      "|[30.0,10744.14,1....|    1|[1.57273298964715...|[0.82817286859469...|       0.0|\n",
      "|[30.0,12788.37,0....|    0|[2.68254534477116...|[0.93598879414199...|       0.0|\n",
      "|[30.0,13473.35,0....|    0|[2.92367208651656...|[0.94900430349779...|       0.0|\n",
      "|[31.0,5387.75,0.0...|    0|[2.62571684167339...|[0.93249844451955...|       0.0|\n",
      "|[31.0,10058.87,1....|    0|[4.27105043710867...|[0.98622528893966...|       0.0|\n",
      "|[31.0,12264.68,1....|    0|[3.38008413978012...|[0.96707628429626...|       0.0|\n",
      "|[32.0,5756.12,0.0...|    0|[4.26970554768203...|[0.98620700670695...|       0.0|\n",
      "|[32.0,9885.12,1.0...|    1|[1.61391559560895...|[0.83395430589729...|       0.0|\n",
      "|[32.0,12142.99,0....|    0|[5.79607394901609...|[0.99696974590616...|       0.0|\n",
      "|[32.0,12403.6,0.0...|    0|[5.80049473949985...|[0.99698307213157...|       0.0|\n",
      "|[32.0,12479.72,0....|    0|[4.77632747766536...|[0.99164352891311...|       0.0|\n",
      "|[33.0,5738.82,0.0...|    0|[4.44444465115443...|[0.98839268592619...|       0.0|\n",
      "|[33.0,7750.54,1.0...|    0|[4.10178087341335...|[0.98372603542335...|       0.0|\n",
      "|[33.0,10306.21,1....|    0|[1.85496074518485...|[0.86470849989654...|       0.0|\n",
      "|[33.0,12115.91,1....|    0|[2.28213478281385...|[0.90738660191322...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of Results with the test data\n",
    "# BinaryClassificationEvaluator expects two inputs, the raw prediction and the label\n",
    "# Its function is to evaluate our model\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "pred_and_labels = fitted_churn_model.evaluate(test_churn)\n",
    "pred_and_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7419416364676713"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using AUC\n",
    "# AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) curve\n",
    "# AUC - ROC curve is a performance measurement for classification problem\n",
    "# It tells how much model is capable of distinguishing between classes\n",
    "\n",
    "churn_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                           labelCol='churn')\n",
    "auc = churn_eval.evaluate(pred_and_labels.predictions)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the model to make predictions on unlabeled data\n",
    "final_lr_model = lr_churn.fit(final_data)\n",
    "\n",
    "# new data to evaluate the model\n",
    "new_customers = spark.read.csv('gs://bucketassignment10/new_customers.csv',inferSchema=True,\n",
    "                              header=True)\n",
    "#schema of our new data\n",
    "new_customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_customers = assembler.transform(new_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_new_customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = final_lr_model.transform(test_new_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|         Company|prediction|\n",
      "+----------------+----------+\n",
      "|        King Ltd|       0.0|\n",
      "|   Cannon-Benson|       1.0|\n",
      "|Barron-Robertson|       1.0|\n",
      "|   Sexton-Golden|       1.0|\n",
      "|        Wood LLC|       0.0|\n",
      "|   Parks-Robbins|       1.0|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_results.select('Company','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are 1.0 values: Cannon-Benson, Barron-Robertson, Sexton-Golden, and Parks-Robbins.\n",
    "# These are the Companies that are likely to churn and that should be assigned managers to\n",
    "# try to prevent them to churn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
