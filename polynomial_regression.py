# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WR--rfsK1-Jl1YDCqmb4Lbi9B8anHUTt


## Data Science
## Rubenia Borge

## Data Structures 

Step 1: Create an appropriate dictionary linking characters to their values.
"""

# Dictionary
my_dict = {'a': 1, 
          'b': 1, 
          'c': 1, 
          'A': 2, 
          'B': 2, 
          'C': 2, 
          '1': 3, 
          '2': 3, 
          '3': 3,          
          '!': 5, 
          '*': 5}  
#print(my_dict)

"""Step 2 : Make a string called mypass and give it the following value: "aabc1!"."""

mypass = "a a b c 1 !"
#print(mypass)

"""Step 3 : Loop to find the total points from the password and print the result."""

charlist = mypass.split()
print(charlist)

total = 0
for i in range(0,6):
  key = charlist[i]
  if key in my_dict:
    total = total + my_dict.get(key)
 
print(total)

"""Step 4 : Divide the total points by the number of letters, and round your result to 3 decimal places."""

division = 0
division = total/len(charlist)
print(division)

"""Step 5 : Now compute that average value (as in Step 4) for each of the following password examples using your code"""

# For "aaaB*c123!"

mypass = "a a a B * c 1 2 3 !"
charlist = mypass.split()
print(charlist)

total = 0
for i in range(0,10):
  key = charlist[i]
  if key in my_dict:
    total = total + my_dict.get(key)
 
print(total)
division = 0
division = total/len(charlist)
print(division)

# For "Cc***13"

mypass = "C c * * * 1 3"
charlist = mypass.split()
print(charlist)

total = 0
for i in range(0,7):
  key = charlist[i]
  if key in my_dict:
    total = total + my_dict.get(key)
 
print(total)
division = 0
division = total/len(charlist)
print(division)

# For "123abcAA"
mypass = "1 2 3 a b c A A"
charlist = mypass.split()
print(charlist)

total = 0
for i in range(0,8):
  key = charlist[i]
  if key in my_dict:
    total = total + my_dict.get(key)
 
print(total)
division = 0
division = total/len(charlist)
print(division)

# For "abcABC123*!"

mypass = "a b c A B C 1 2 3 * !"
charlist = mypass.split()
print(charlist)

total = 0
for i in range(0,11):
  key = charlist[i]
  if key in my_dict:
    total = total + my_dict.get(key)
 
print(total)
division = 0
division = total/len(charlist)
print(division)

"""## Data Visualization and Polynomial Regression (5 points)

Step : Create a function that generates a number according to the following equation: 
y = np.cos(1.5 * np.pi * x) + 0.1*np.random.normal(0,1,1)
"""

from random import randint
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn import linear_model
from sklearn.metrics import mean_squared_error
import math 
from math import sqrt

def y_generator(x):
    return np.cos(1.5 * np.pi * x) + 0.1*np.random.normal(0,1,1)

print(y_generator(10))

"""Step 2 : Generate two samples with x randomly chosen between 0 and 1.
A training set of 30 (x,y) pairs
A test set of 100 (x,y) pairs

2.A A training set of 30 (x,y) pairs
"""

# Commented out IPython magic to ensure Python compatibility.
#Allows a tuple of values to be used as input
f2 = np.vectorize(y_generator)

np.random.seed(100)

#Getting 10 x-values from the x data set
#X_training = np.random.uniform(0,3,10)
X_training = np.sort(np.random.uniform(0,1,30))
#Getting 10 y-values from the y data set that has been computed 
y_training = f2(X_training)

print("----X values for training set----")
print(X_training)

print()
print("----Y values for training set----")
print(y_training)

# %matplotlib inline
#Plots the data points in the training set
#plt.xlim(0,3)
#plt.ylim(0,1.3)
plt.plot(X_training,y_training,'o')
plt.xlabel('X-values for Training Set')
plt.ylabel('Y-values for Training Set')
plt.title('Y vs. X for Training Set')
plt.show()

"""2.B A test set of 100 (x,y) pairs"""

# Commented out IPython magic to ensure Python compatibility.
X_test = np.random.uniform(0,1,100)
y_test = f2(X_test)

print("----X values for test set----")
print(X_test)

print()
print("----Y values for training set----")
print(y_test)

# %matplotlib inline
#Plots the data points in the test set
#plt.xlim(0,3)
#plt.ylim(0,1.3)
plt.plot(X_test,y_test,'o')
plt.xlabel('X-values for Test Set')
plt.ylabel('Y-values for Test Set')
plt.title('Y vs. X for Test Set')
plt.show()

"""Step 3 : Fit the following polynomials to the training set:
Degree 1 polynomial 
Degree 4 polynomial 
Degree 15 polynomial

"""

X_new = np.linspace(0,3)
#print(X_new)

model1 = Pipeline([('poly', PolynomialFeatures(degree=1)),('linear', linear_model.LinearRegression())])
model1 =model1.fit(X_training[:,np.newaxis], y_training[:,np.newaxis])

y_new1 = model1.predict(X_new[:, np.newaxis])


#Plotting the degree 1 fit on the test data
plt.scatter(X_training, y_training)
plt.plot(X_new, y_new1, 'r', label="Fit "+str(1)+ " degree poly")

plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,
               ncol=2, mode="expand", borderaxespad=0.)
#plt.xlim(0,3)
#plt.ylim(0,1.3)

plt.show()

model4 = Pipeline([('poly', PolynomialFeatures(degree=4)),('linear', linear_model.LinearRegression())])
model4 =model4.fit(X_training[:,np.newaxis], y_training[:,np.newaxis])

y_new2 = model4.predict(X_new[:, np.newaxis])

#Plotting the degree 1 fit on the test data
plt.scatter(X_training, y_training)
plt.plot(X_new, y_new2, 'r', label="Fit "+str(4)+ " degree poly")
plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,
               ncol=2, mode="expand", borderaxespad=0.)
#plt.xlim(0,3)
#plt.ylim(0,1.3)

plt.show()

model15 = Pipeline([('poly', PolynomialFeatures(degree=15)),('linear', linear_model.LinearRegression())])
model15 =model15.fit(X_training[:,np.newaxis], y_training[:,np.newaxis])

y_new9 = model15.predict(X_new[:, np.newaxis])

#Plotting the degree 1 fit on the test data
plt.scatter(X_training, y_training)
plt.plot(X_new, y_new9, 'r', label="Fit "+str(15)+ " degree poly")
plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,
               ncol=2, mode="expand", borderaxespad=0.)
#plt.xlim(0,3)
plt.ylim(0,1.3)

plt.show()

"""Step 4 (1 pt): Create a plot showing the original data as a scatter plot and the three polynomial fits with a legend indicating the degree of the polynomial.

"""

#Creating sin function
sin_func = np.sin(X_new)
#plot sin function and gives label for legend
plt.plot(X_new,sin_func,'p:',label='Sin curve')

plot_config=['b', 'r', 'y']
plt.plot(X_training, y_training, 'go', label="Actual")
# 3. Set the polynomial degree to be fitted betwee 1 and 3
d_degree = [1,2,9]
i = 0
for degree in d_degree:
    # 5. Create a fit a polynomial with sk-learn LinearRegression
    model = Pipeline([('poly', PolynomialFeatures(degree=degree)),('linear', linear_model.LinearRegression())])
    model=model.fit(X_training[:,np.newaxis], y_training[:,np.newaxis])
    
    predict_sk=model.predict(X_new[:,np.newaxis])
    
    plt.plot(X_new, predict_sk, plot_config[i], label="Fit "+str(degree)+ " degree poly")
    plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,
               ncol=2, mode="expand", borderaxespad=0.)
    #plt.xlim(0,3)
    plt.ylim(0,1.3)
    i = i+1

plt.show()

"""Step 5 (1 pt): Apply each of the 3 models to predict the y-value from the x-value in the test set. Find the root mean square error between the predictions and the true values.

"""

#Root mean squared error 
y_predictions1 = model1.predict(X_test[:, np.newaxis])
rms1 = sqrt(mean_squared_error(y_test,y_predictions1))
print('Root mean square error for Degree 1')
print(rms1)


#Root mean squared error 
y_predictions4 = model4.predict(X_test[:, np.newaxis])
rms4 = sqrt(mean_squared_error(y_test,y_predictions4))
print('Root mean square error for Degree 4')
print(rms4)


#Root mean squared error 
y_predictions15 = model15.predict(X_test[:, np.newaxis])
rms15 = sqrt(mean_squared_error(y_test,y_predictions15))
print('Root mean square error for Degree 15')
print(rms15)

"""## Part 3: DataFrames and KNN Classification (7 points)

Step 1 (1 pt): Download and read in this CSV file on individual titanic passengers into a dataframe, remove any entries that do not include the age of the person (indicated as “NA” in the file), and keep only the columns PClass, Age, Survived, and SexCode. Name the data frame as titanic_df.
"""

import pandas as pd
import numpy as np
#12 Reading data from a csv file
titanicdf = pd.read_csv('titanic_individuals.csv')
print(titanicdf)

# Remove data that has NA

titanic_df = titanicdf.dropna()
print(titanic_df)

"""Step 2 (2 pts): Now use titanic_df to calculate the percent that survived in each subgroup below. Assume adults are 18 or older.

"""

# number of people in the dataset without NA data
total_people = titanic_df['Name'].count()
print("The total number of people is: ")
print(total_people)
people = 756
# total number of children 
total_children_df = titanic_df[(titanic_df['Age'] < 18)]
print("The total number of children is: ")
print(total_children_df['Name'].count())
#total_children = total_children_df['Name'].count()
#print(total_chidren)
children = 96
adults = people - children
print("The number of adults is: ")
print(adults)
print("\n")

my_children_df = titanic_df[((titanic_df['Age'] < 18) & (titanic_df['Survived'] == 1))]
grouped = children_df.groupby('PClass')
print("Percentage of Children who Survived per Class")
print("Survived", ((grouped['Survived'].count())/children)*100)
print("\n") 

my_adult_df = titanic_df[((titanic_df['Age'] > 18) & (titanic_df['Survived'] == 1))]
grouped = my_adult_df.groupby('PClass')
print("Percentage of Adults who Survived per Class")
print("Survived", ((grouped['Survived'].count())/adults)*100)

"""Step 3 (1 pt): Now, only using the following training_data and training_target data sets, create a survival classifier given someone’s class, age, and sex. 
Use a kNN classifier with k=1, then apply it to a simple test case of your choosing.

"""

# This is the training data

training_data = [ [1, 60, 0],
                  [2, 20, 0],
                  [3, 10, 1],
                  [1,  5, 0],
                  [2, 50, 1],
                  [3, 25, 1],
                  [1, 55, 0],
                  [2, 12, 1],
                  [3, 45, 0] ]

print(training_data)

# This is the given training_target data

training_target = [1, 0, 1, 1, 1, 0, 0, 1, 0]

print(training_target)

# Use a kNN classifier with k=1, then apply it to a simple test case of your choosing.

# This is the simple test case of my choosing.
my_own_test_data = [ [2, 35, 1],
                  [3, 60, 0],
                  [1, 45, 0],
                  [1,  40, 1],
                  [2, 18, 1],
                  [1, 20, 0]]  


from sklearn import neighbors

#k-NN classifier for k=1
#By using ‘distance’, closer neighbors will have greater weight #than further ones
k1 = neighbors.KNeighborsClassifier(n_neighbors=1, weights='distance')
k1.fit(training_data, training_target)
# apply the model to the test data
k1_pred = k1.predict(my_own_test_data)
print(k1_pred)

"""Step 4 (1 pt): Now create and print a test_data array with the following cases from first to last in this order.

And use your classifier from Step 3 on this test_data array to make a prediction on each of these examples. State clearly which are predicted to survive. 

"""

# This is the test data given in the exam
test_data = [ [1, 5, 0],
                  [1, 60, 1],
                  [2, 25, 0],
                  [2,  40, 1],
                  [3, 8, 1],
                  [3, 30, 0]] 

# Use your classifier from Step 3 on this test_data array to make a prediction on each of these examples. 
# State clearly which are predicted to survive.

from sklearn import neighbors

#k-NN classifier for k=1
#By using ‘distance’, closer neighbors will have greater weight #than further ones
k1 = neighbors.KNeighborsClassifier(n_neighbors=1, weights='distance')
k1.fit(training_data, training_target)
# apply the model to the test data
k1_pred = k1.predict(test_data)
print(k1_pred)

# A 5 year old boy in first class      --SURVIVED
# A 60 year old woman in first class   --SURVIVED
# A 25 year old man in second class    --NOT SURVIVED
# A 40 year old woman in second class  --NOT SURVIVED
# A 8 year old girl in third class     --SURVIVED
# A 30 year old man in third class     --NOT SURVIVED

"""Step 5 (1 pt): Make your own training_data and training_target data sets FROM THE REAL DATA in the titanic_df data frame."""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


my_data = pd.read_csv("training_data_knn.csv") 
df = pd.DataFrame({'Class': my_data['Class'], 'Age': my_data['Age'],'SexCode': my_data['SexCode']})

#Converting the x-training list into a 1D array
training_data_knn = np.array(df)
#print(training_data_knn)

target = pd.read_csv("training_target_knn.csv") 
dftarget = pd.DataFrame({'Survived': target['Survived']}).to_numpy()
target_data_knn = np.array(dftarget)

#print(target_data_knn)

"""Step 6 : Now build a kNN classifier using training_data and training_target data sets from Step 5. Use k=3. Apply that classifier to the test_data array created in Step 4, and briefly discuss the results."""

#k-NN classifier for k=1
#By using ‘distance’, closer neighbors will have greater weight #than further ones
k1 = neighbors.KNeighborsClassifier(n_neighbors=3, weights='distance')
k1.fit(training_data_knn, target_data_knn)
# apply the model to the test data
k1_pred = k1.predict(test_data)
print(k1_pred)

# A 5 year old boy in first class      --SURVIVED
# A 60 year old woman in first class   --SURVIVED
# A 25 year old man in second class    --NOT SURVIVED
# A 40 year old woman in second class  --SURVIVED
# A 8 year old girl in third class     --SURVIVED
# A 30 year old man in third class     --NOT SURVIVED

# The difference is not extreme but there are some differences than when I was training with less
# training data. Also I would need to add more test data. The more data I have
# The more accurate results I will have. I would need to call accuracy and precision in 
# this model but it takes time for me because I am just starting in this field.
# But yes the difference comes become now I trained my model with more data
# And in general this tend to increase the accuracy of my predictions.